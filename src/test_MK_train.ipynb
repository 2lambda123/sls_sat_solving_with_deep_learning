{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 14:52:32.873166: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:\n",
      "\n",
      "  slice.62 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).\n",
      "\n",
      "This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.\n",
      "\n",
      "If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "2022-12-20 14:52:33.624153: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 1.751174s\n",
      "Constant folding an instruction is taking > 1s:\n",
      "\n",
      "  slice.62 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).\n",
      "\n",
      "This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.\n",
      "\n",
      "If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import optax\n",
    "import haiku as hk\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "from data_utils import SATTrainingDataset, JraphDataLoader\n",
    "from model import network_definition\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "f=0.0001\n",
    "\n",
    "# # Make a batched version of the forwarding\n",
    "# batched_predict = jax.vmap(network.apply, in_axes=(None, 0))\n",
    "\n",
    "\n",
    "# def loss(params, problems, targets):\n",
    "#     preds = batched_predict(params, problems)\n",
    "#     return -jnp.mean(preds * targets)\n",
    "\n",
    "\n",
    "def one_hot(x, k, dtype=jnp.float32):\n",
    "    \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "    return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
    "\n",
    "vmap_one_hot=jax.vmap(one_hot, in_axes=(0,None), out_axes=0)\n",
    "\n",
    "path='/Users/p403830/Library/CloudStorage/OneDrive-PorscheDigitalGmbH/programming/ml_based_sat_solver/BroadcastTestSet_subset/'\n",
    "sat_data = SATTrainingDataset(path)\n",
    "train_data, test_data = data.random_split(sat_data, [0.8, 0.2])\n",
    "\n",
    "train_loader = JraphDataLoader(train_data, batch_size=1, shuffle=True)\n",
    "\n",
    "network = hk.without_apply_rng(hk.transform(network_definition))\n",
    "params = network.init(jax.random.PRNGKey(42), sat_data[0][0].graph)\n",
    "\n",
    "opt_init, opt_update = optax.adam(1e-3)\n",
    "opt_state = opt_init(params)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def compute_log_probs(decoded_nodes, mask, candidate):\n",
    "            a=jax.nn.log_softmax(decoded_nodes) * mask[:, None]\n",
    "            b= jnp.dot(candidate,a.T)\n",
    "            return b\n",
    "\n",
    "\n",
    "vmap_compute_log_probs=jax.vmap(compute_log_probs, in_axes=(None,None, 0), out_axes=0)\n",
    "\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update(params, opt_state, x, y, f):\n",
    "            batch_masks=x[0]\n",
    "            batch_graphs=x[1]\n",
    "            batch_c=y[0]\n",
    "            batch_e=y[1]\n",
    "            #print(len(batch_masks))\n",
    "            #print(len(batch_graphs))\n",
    "            #print(len(batch_c))\n",
    "            #print(len(batch_e))\n",
    "            #print(batch_e[0])\n",
    "            #print(batch_e)\n",
    "            #loss=new_prediction_loss(params, batch_masks[0], batch_graphs[0], batch_c[0], batch_e[0], f)\n",
    "            #print(loss)\n",
    "            g=jax.grad(new_prediction_loss)(params, batch_masks[0], batch_graphs[0], batch_c[0], batch_e[0], f)\n",
    "            #g=jax.grad(batched_loss)(params, *x, c, e, f)\n",
    "            ####\n",
    "            updates, opt_state = opt_update(g, opt_state)\n",
    "            return optax.apply_updates(params, updates), opt_state\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def prediction_loss(params, mask, graph, solution):\n",
    "        decoded_nodes = network.apply(params, graph)\n",
    "        solution = one_hot(solution, 2)\n",
    "        # We interpret the decoded nodes as a pair of logits for each node.\n",
    "        log_prob = jax.nn.log_softmax(decoded_nodes) * solution\n",
    "        return -jnp.sum(log_prob * mask[:, None]) / jnp.sum(mask)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def new_prediction_loss(params, mask, graph, candidates, energies, f: float):\n",
    "            decoded_nodes = network.apply(params, graph)\n",
    "            candidates = vmap_one_hot(candidates, 2)\n",
    "            log_prob=vmap_compute_log_probs(decoded_nodes, mask, candidates)\n",
    "            weights = jax.nn.softmax(- f * energies)\n",
    "            #print(np.shape(weights))\n",
    "            weighted_log_probs = jax.vmap(jnp.dot,axis_name=(0,0), out_axes=0)(log_prob, weights)\n",
    "            #print(np.shape(weighted_log_probs))\n",
    "            summed_weighted_log_probs=jnp.sum(weighted_log_probs, axis=0) #sum over all candidates\n",
    "            #print(np.shape(summed_weighted_log_probs))\n",
    "            #loss=1\n",
    "            print(np.shape(summed_weighted_log_probs))\n",
    "            print(np.shape(mask[:,None]))\n",
    "            a=jnp.dot(summed_weighted_log_probs ,mask[:, None])\n",
    "            b=jnp.asarray(a)\n",
    "            print(b)\n",
    "            loss=-np.sum(b) #/ jnp.sum(mask)\n",
    "            print(loss)\n",
    "            #print(np.shape(loss))\n",
    "            #print(loss)\n",
    "            return loss#summed_weighted_log_probs#loss\n",
    "\n",
    "#batched_loss = jax.vmap(new_prediction_loss_single, in_axes=(None, None, None, 1,1, None), out_axes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_number 1\n",
      "[  1 281 339 352 282]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "counter=0\n",
    "for (batch_p, batch_ce) in train_loader:\n",
    "            counter=counter+1\n",
    "            print(\"batch_number\", counter)\n",
    "            batch_masks=batch_p[0]\n",
    "            batch_graphs=batch_p[1]\n",
    "            batch_c=batch_ce[0]\n",
    "            batch_e=batch_ce[1]\n",
    "            #print(len(batch_masks))\n",
    "            #print(len(batch_graphs))\n",
    "            #print(len(batch_c))\n",
    "            #print(len(batch_e))\n",
    "            #print(batch_e)\n",
    "            mask = batch_masks[0]\n",
    "            graph = batch_graphs[0]\n",
    "            c = batch_c[0]\n",
    "            e= batch_e[0]\n",
    "            print(e)\n",
    "            f=0.0001\n",
    "            decoded_nodes = network.apply(params, graph)\n",
    "            candidates = vmap_one_hot(c, 2)\n",
    "            #log_prob=compute_log_probs(decoded_nodes, mask, candidates[0])\n",
    "            #log_prob= jax.vmap(compute_log_probs, in_axes=(None,None, 0), out_axes=0)(decoded_nodes, mask, candidates)\n",
    "            log_prob=vmap_compute_log_probs(decoded_nodes, mask, candidates)\n",
    "            weights = jax.nn.softmax(- f * e)\n",
    "            #weights= scipy.special.softmax(-f*e)\n",
    "            print(weights)\n",
    "            #print(np.shape(weights))\n",
    "            weighted_log_probs = jax.vmap(jnp.dot,axis_name=(0,0), out_axes=0)(log_prob, weights)\n",
    "            print(np.shape(weighted_log_probs))\n",
    "            summed_weighted_log_probs=jnp.sum(weighted_log_probs, axis=0) #sum over all candidates\n",
    "            print(np.shape(summed_weighted_log_probs))\n",
    "            print(summed_weighted_log_probs)\n",
    "            #loss=1\n",
    "            #print(np.shape(summed_weighted_log_probs))\n",
    "            #print(np.shape(mask[:,None]))\n",
    "            #a=jnp.dot(summed_weighted_log_probs ,mask[:, None])\n",
    "            #b=jnp.asarray(a)\n",
    "            #print(b)\n",
    "            #loss=-np.sum(b) #/ jnp.sum(mask)\n",
    "            #print(loss)\n",
    "\n",
    "\n",
    "\n",
    "            #loss=new_prediction_loss(params, batch_masks[0], batch_graphs[0], batch_c[0], batch_e[0], f)\n",
    "            #print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=1\n",
    "print(np.shape(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering training loop\n",
      "epoch 1\n",
      "batch_number 1\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Entering training loop\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "        print(\"epoch\", epoch+1)\n",
    "        start_time = time.time()\n",
    "        counter=0\n",
    "        for (batch_p, batch_ce) in train_loader:\n",
    "            counter=counter+1\n",
    "            print(\"batch_number\", counter)\n",
    "            #batch_masks=batch_p[0]\n",
    "            #batch_graphs=batch_p[1]\n",
    "            #batch_c=batch_ce[0]\n",
    "            #batch_e=batch_ce[1]\n",
    "            #print(len(batch_masks))\n",
    "            #print(len(batch_graphs))\n",
    "            #print(len(batch_c))\n",
    "            #print(len(batch_e))\n",
    "            #print(batch_e)\n",
    "            params, opt_state = update(params, opt_state, batch_p, batch_ce, f)\n",
    "            print(counter, \"done\")\n",
    "        '''\n",
    "        for (p, ce) in train_loader:\n",
    "            #c=ce[0]\n",
    "            #e=ce[1]\n",
    "            for i in range(0,len(ce[1])):#,len(e)):\n",
    "                params, opt_state = update(params, opt_state, p, ce[:][i], f)\n",
    "            print(f\"{batch} done\")\n",
    "        epoch_time = time.time() - start_time\n",
    "        '''\n",
    "        # train_acc = accuracy(params, train_images, train_labels)\n",
    "        # test_acc = accuracy(params, test_images, test_labels)\n",
    "        #print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "\n",
    "        #test_acc = jnp.mean(jnp.asarray([prediction_loss(params, p.mask, p.graph, s) for (p, s) in test_data]))\n",
    "\n",
    "        #TBD!!!\n",
    "\n",
    "        test_acc = jnp.mean(jnp.asarray([new_prediction_loss(p, p.graph, c, f) for (p, c) in test_data]))\n",
    "        \n",
    "        ##\n",
    "\n",
    "        # print(\"Training set accuracy {}\".format(train_acc))\n",
    "        print(\"Test set accuracy {}\".format(test_acc))\n",
    "\n",
    "    # TODO: Save the model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd78fef2128015050713e82ca51c6520b11aee7c9ee8df750520bbbc7384cbaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
