{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "840cbe77",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Implementation of deep learning based moser-type CSP solver\n",
    "\n",
    "Here we basically combine the approach from the Deepmind paper \"MIP solving using deep neural networks\" with \" a local lemma for focussed stochastic search\" from Achlioptas.\n",
    "\n",
    "## Defining Atomic CSPs and the random walker\n",
    "\n",
    "We focus on binary problems first (basically these are SAT problems I think)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "bdff3658",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import NamedTuple\n",
    "import numpy.typing as npt\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.transforms as T\n",
    "from pysat.formula import CNF\n",
    "\n",
    "class AtomicConstraint:\n",
    "    \n",
    "    def __init__(self,support, vals):\n",
    "        self.support: npt.NDArray[np.int_] = support\n",
    "        self.vals: npt.NDArray[np.int_] = vals\n",
    "\n",
    "    def is_violated_by(self, assignment: npt.NDArray[np.int_]):\n",
    "        return np.array_equiv(self.vals,assignment[self.support])\n",
    "      \n",
    "    \n",
    "class BinAtCSP:\n",
    "    variables: npt.NDArray[np.int_]\n",
    "    constraints: list[AtomicConstraint]\n",
    "\n",
    "    def num_vars(self):\n",
    "        self = len(self.variables)\n",
    "    \n",
    "    def from_cnf(self, cnf: CNF):\n",
    "        self.variables = np.arange(cnf.nv)\n",
    "        self.constraints = [\n",
    "            AtomicConstraint(\n",
    "                support=np.array([(abs(l) - 1) for l in c]), \n",
    "                vals=np.array(((np.sign(c) + 1) // 2))\n",
    "            ) for c in cnf.clauses if len(c) > 0\n",
    "        ]\n",
    "        assert all(len(c.support) == len(c.vals) for c in self.constraints)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def to_pytorch(self):\n",
    "        edges = []\n",
    "        edge_features = []\n",
    "        n = len(self.variables)\n",
    "        m = len(self.constraints)\n",
    "        \n",
    "        for j,c in enumerate(self.constraints, start=n):\n",
    "            edge_features.extend(c.vals)\n",
    "            edges.extend([v, j] for v in c.support)   \n",
    "\n",
    "        # index connecting variable nodes and constraint nodes\n",
    "        edge_index = torch.tensor(edges)\n",
    "\n",
    "        # edge attributes: A matrix of shape [num_edges, 1] that contains for each edge the element of the constraint\n",
    "        edge_attr = torch.tensor(edge_features, dtype=torch.float)\n",
    "\n",
    "        # node attributes will be initialised to 1.\n",
    "        x = torch.ones(n + m,1)\n",
    "\n",
    "        # define the graph\n",
    "        data = Data(x=x, edge_index=edge_index.t().contiguous(), edge_attr=edge_attr)\n",
    "        data = T.ToUndirected()(data)\n",
    "        return data\n",
    "            \n",
    "class Oracle:\n",
    "    \n",
    "    def __init__(self, dist, params):\n",
    "        self.dist = dist\n",
    "        self.params = params\n",
    "\n",
    "    def sample(self, support: npt.NDArray[np.int_] ):\n",
    "        conditional_dist = self.dist(logits=self.params[support])\n",
    "        return conditional_dist.sample().flatten()\n",
    "    \n",
    "    \n",
    "class OracleRandomWalker:\n",
    "    \n",
    "    def __init__(self, oracle: Oracle, instance: BinAtCSP):\n",
    "        self.oracle = oracle\n",
    "        self.instance = instance\n",
    "        \n",
    "    def find_violated_constraint(self, assignment: npt.NDArray[np.int_]):\n",
    "        # to accelerate this: \n",
    "        \n",
    "        for i, constraint in enumerate(self.instance.constraints):\n",
    "            if constraint.is_violated_by(assignment):\n",
    "                return constraint, True\n",
    "        return None, False\n",
    "    \n",
    "    def run(self,step_limit=1000, return_trajectory=False):\n",
    "        assignment = self.oracle.sample(self.instance.variables)\n",
    "        trajectory = assignment if return_trajectory else None\n",
    "        counter = 1\n",
    "        while counter < step_limit:\n",
    "            flaw, flaw_exists = self.find_violated_constraint(assignment.numpy())\n",
    "            if not flaw_exists:\n",
    "                return assignment, \"satisfied\", counter, trajectory\n",
    "                \n",
    "            # update assignment\n",
    "            assignment[flaw.support] = self.oracle.sample(support = flaw.support)\n",
    "            if return_trajectory:\n",
    "                trajectory = torch.vstack([trajectory, assignment])\n",
    "            counter += 1\n",
    "            \n",
    "        return assignment, \"unsatisfied\", counter, trajectory\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e03f8f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define the neural network\n",
    "\n",
    "Following the GCN + MLP generative apprpoach from the Deepmind paper here. Since we deal with CSPs instead of MIPs, the encoding is slightly different: I assign the violated bitstrings to edges and then pool them in a first step to generate node embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "bc0d4636",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn as nn\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, out_dim = 20):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(1, 1, normalize=True, add_self_loops=False)\n",
    "        self.conv2 = GCNConv(1, 16)\n",
    "        self.conv3 = GCNConv(16, out_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.conv1(data.x, data.edge_index, edge_weight=data.edge_attr)\n",
    "        x = self.conv2(x, data.edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, data.edge_index)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "7bd902dd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# defining the MLP \n",
    "class MLP(nn.Module):\n",
    "    # the input here should be the final node embedding dimension\n",
    "    def __init__(self, in_dim=20, hidden_n_2=400):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_n_2)\n",
    "        self.fc2 = nn.Linear(hidden_n_2, 1)\n",
    "\n",
    "        self.reLU = nn.ReLU() \n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, embedded):\n",
    "        h1 = self.reLU(self.fc1(embedded))\n",
    "        return self.fc2(h1) # self.sigmoid(self.fc2(h1))\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, latent_dim=20):\n",
    "        super(Model, self).__init__()\n",
    "        self.decoder = MLP(in_dim=latent_dim)\n",
    "        self.gcn = GCN(out_dim=latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.gcn(x)\n",
    "        return self.decoder(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399706fd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training\n",
    "\n",
    "How do we traing this model? In other words, what is the loss as a function of the current model? WE follow two approaches, one unsupervised, the other supervised. \n",
    "\n",
    "In both cases, we follow Deepmind in trying to minimize the cross entropy between the generative distribution and a Gibbs distribution that favors low energy state. However, our energy function is different from their linear function. \n",
    "\n",
    "For the supervised, we do exactly what Deepmind did and use an off-the-shelf solver to find solutions and then we estimate their loss function using the trajectory of the solver (so including sub-optimal solutions). We use these points estimate the energy function and then get a differentiable estimate of the cross entropy.\n",
    "\n",
    "Unsupervised: For given parameters, we run the random walker for $N$ steps and then use this to estimate the cross entropy. This also gives us a differentiable estimate of the loss function (with the subtlety that now the energy estimates also depend on the parameters so we need some kind of reparametrization trick to deal with that, maybe we can also do this analytically??  \n",
    "\n",
    "### Loading the training instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "b08c1069",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "instances = [BinAtCSP().from_cnf(CNF(from_file=f)) for f in glob.glob('uf20-91/*.cnf')]\n",
    "\n",
    "train_instances, test_instances = instances[:800], instances[800:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa74dbb7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Implementing the unsupervised approach\n",
    "\n",
    "Lets first do it without consider the fact that our weights will depend on the parameters (this might lead to bad performance, but lets see)\n",
    "\n",
    "#### Defining the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "1a111216",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def energy(assignment, instance):\n",
    "    # this is a bit of a random energy function and to be investigated/improved\n",
    "    p = np.sum([c.is_violated_by(assignment) for c in instance.constraints])/len(instance.constraints)\n",
    "    return np.maximum(np.sqrt(p), 1e-12)\n",
    "\n",
    "\n",
    "class UnsupervisedLoss(nn.Module):\n",
    "    def __init__(self, number_samples = 100, temperature = 1, alpha=3):\n",
    "        super(UnsupervisedLoss, self).__init__()\n",
    "        \n",
    "        self.bce_loss = nn.BCEWithLogitsLoss(reduction='none')\n",
    "        self.random_walker = OracleRandomWalker\n",
    "        self.number_samples = number_samples\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def forward(self, params, instance):\n",
    "        variable_params = params[:len(instance.variables)]\n",
    "        oracle = Oracle(dist=Bernoulli, params=variable_params.detach())\n",
    "        walker = self.random_walker(oracle, instance)\n",
    "        _, _, _, trajectory = walker.run(step_limit=self.number_samples, return_trajectory=True)\n",
    "        weights = torch.Tensor(self.estimate_gibbs_weights(trajectory, instance))\n",
    "        neg_log_probs_model = torch.sum(self.bce_loss(variable_params.tile(self.number_samples).T, trajectory), axis=1)\n",
    "        \n",
    "        return weights.dot(neg_log_probs_model)\n",
    "    \n",
    "    def estimate_gibbs_weights(self, trajectory, instance):\n",
    "        trajectory = trajectory.numpy()\n",
    "        energies = np.apply_along_axis(func1d=energy, arr=trajectory, instance=instance, axis=1)\n",
    "        weights = np.exp(- self.temperature * energies)\n",
    "        return weights/np.sum(weights)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "53c1f318",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train on instance 0\n",
      "loss: 15.110449  [    0/  800]\n",
      "Starting to train on instance 1\n",
      "Starting to train on instance 2\n",
      "Starting to train on instance 3\n",
      "Starting to train on instance 4\n",
      "Starting to train on instance 5\n",
      "Starting to train on instance 6\n",
      "Starting to train on instance 7\n",
      "Starting to train on instance 8\n",
      "Starting to train on instance 9\n",
      "Starting to train on instance 10\n",
      "loss: 12.845843  [   10/  800]\n",
      "Starting to train on instance 11\n",
      "Starting to train on instance 12\n",
      "Starting to train on instance 13\n",
      "could not train on instance 13\n",
      "Starting to train on instance 14\n",
      "Starting to train on instance 15\n",
      "Starting to train on instance 16\n",
      "Starting to train on instance 17\n",
      "Starting to train on instance 18\n",
      "Starting to train on instance 19\n",
      "could not train on instance 19\n",
      "Starting to train on instance 20\n",
      "loss: 13.869633  [   20/  800]\n",
      "Starting to train on instance 21\n",
      "Starting to train on instance 22\n",
      "Starting to train on instance 23\n",
      "Starting to train on instance 24\n",
      "Starting to train on instance 25\n",
      "Starting to train on instance 26\n",
      "Starting to train on instance 27\n",
      "Starting to train on instance 28\n",
      "Starting to train on instance 29\n",
      "could not train on instance 29\n",
      "Starting to train on instance 30\n",
      "loss: 13.833910  [   30/  800]\n",
      "Starting to train on instance 31\n",
      "could not train on instance 31\n",
      "Starting to train on instance 32\n",
      "Starting to train on instance 33\n",
      "Starting to train on instance 34\n",
      "Starting to train on instance 35\n",
      "Starting to train on instance 36\n",
      "Starting to train on instance 37\n",
      "Starting to train on instance 38\n",
      "Starting to train on instance 39\n",
      "Starting to train on instance 40\n",
      "loss: 13.549910  [   40/  800]\n",
      "Starting to train on instance 41\n",
      "Starting to train on instance 42\n",
      "could not train on instance 42\n",
      "Starting to train on instance 43\n",
      "could not train on instance 43\n",
      "Starting to train on instance 44\n",
      "could not train on instance 44\n",
      "Starting to train on instance 45\n",
      "Starting to train on instance 46\n",
      "Starting to train on instance 47\n",
      "Starting to train on instance 48\n",
      "Starting to train on instance 49\n",
      "Starting to train on instance 50\n",
      "loss: 14.138391  [   50/  800]\n",
      "Starting to train on instance 51\n",
      "Starting to train on instance 52\n",
      "Starting to train on instance 53\n",
      "Starting to train on instance 54\n",
      "Starting to train on instance 55\n",
      "Starting to train on instance 56\n",
      "Starting to train on instance 57\n",
      "Starting to train on instance 58\n",
      "Starting to train on instance 59\n",
      "could not train on instance 59\n",
      "Starting to train on instance 60\n",
      "loss: 13.421563  [   60/  800]\n",
      "Starting to train on instance 61\n",
      "Starting to train on instance 62\n",
      "Starting to train on instance 63\n",
      "Starting to train on instance 64\n",
      "Starting to train on instance 65\n",
      "Starting to train on instance 66\n",
      "Starting to train on instance 67\n",
      "Starting to train on instance 68\n",
      "Starting to train on instance 69\n",
      "Starting to train on instance 70\n",
      "loss: 13.942384  [   70/  800]\n",
      "Starting to train on instance 71\n",
      "Starting to train on instance 72\n",
      "Starting to train on instance 73\n",
      "Starting to train on instance 74\n",
      "Starting to train on instance 75\n",
      "Starting to train on instance 76\n",
      "Starting to train on instance 77\n",
      "Starting to train on instance 78\n",
      "could not train on instance 78\n",
      "Starting to train on instance 79\n",
      "Starting to train on instance 80\n",
      "loss: 13.591677  [   80/  800]\n",
      "Starting to train on instance 81\n",
      "Starting to train on instance 82\n",
      "Starting to train on instance 83\n",
      "Starting to train on instance 84\n",
      "could not train on instance 84\n",
      "Starting to train on instance 85\n",
      "Starting to train on instance 86\n",
      "Starting to train on instance 87\n",
      "Starting to train on instance 88\n",
      "Starting to train on instance 89\n",
      "Starting to train on instance 90\n",
      "loss: 13.958045  [   90/  800]\n",
      "Starting to train on instance 91\n",
      "Starting to train on instance 92\n",
      "could not train on instance 92\n",
      "Starting to train on instance 93\n",
      "Starting to train on instance 94\n",
      "Starting to train on instance 95\n",
      "Starting to train on instance 96\n",
      "Starting to train on instance 97\n",
      "Starting to train on instance 98\n",
      "Starting to train on instance 99\n",
      "Starting to train on instance 100\n",
      "loss: 13.876190  [  100/  800]\n",
      "Starting to train on instance 101\n",
      "Starting to train on instance 102\n",
      "Starting to train on instance 103\n",
      "Starting to train on instance 104\n",
      "Starting to train on instance 105\n",
      "Starting to train on instance 106\n",
      "Starting to train on instance 107\n",
      "Starting to train on instance 108\n",
      "Starting to train on instance 109\n",
      "Starting to train on instance 110\n",
      "loss: 13.790132  [  110/  800]\n",
      "Starting to train on instance 111\n",
      "could not train on instance 111\n",
      "Starting to train on instance 112\n",
      "Starting to train on instance 113\n",
      "Starting to train on instance 114\n",
      "Starting to train on instance 115\n",
      "Starting to train on instance 116\n",
      "Starting to train on instance 117\n",
      "Starting to train on instance 118\n",
      "Starting to train on instance 119\n",
      "Starting to train on instance 120\n",
      "loss: 13.550133  [  120/  800]\n",
      "Starting to train on instance 121\n",
      "Starting to train on instance 122\n",
      "Starting to train on instance 123\n",
      "Starting to train on instance 124\n",
      "Starting to train on instance 125\n",
      "Starting to train on instance 126\n",
      "Starting to train on instance 127\n",
      "Starting to train on instance 128\n",
      "Starting to train on instance 129\n",
      "Starting to train on instance 130\n",
      "could not train on instance 130\n",
      "Starting to train on instance 131\n",
      "Starting to train on instance 132\n",
      "Starting to train on instance 133\n",
      "Starting to train on instance 134\n",
      "Starting to train on instance 135\n",
      "Starting to train on instance 136\n",
      "Starting to train on instance 137\n",
      "Starting to train on instance 138\n",
      "Starting to train on instance 139\n",
      "Starting to train on instance 140\n",
      "loss: 13.317383  [  140/  800]\n",
      "Starting to train on instance 141\n",
      "Starting to train on instance 142\n",
      "Starting to train on instance 143\n",
      "Starting to train on instance 144\n",
      "Starting to train on instance 145\n",
      "Starting to train on instance 146\n",
      "Starting to train on instance 147\n",
      "Starting to train on instance 148\n",
      "Starting to train on instance 149\n",
      "Starting to train on instance 150\n",
      "loss: 14.119629  [  150/  800]\n",
      "Starting to train on instance 151\n",
      "Starting to train on instance 152\n",
      "Starting to train on instance 153\n",
      "Starting to train on instance 154\n",
      "Starting to train on instance 155\n",
      "Starting to train on instance 156\n",
      "Starting to train on instance 157\n",
      "Starting to train on instance 158\n",
      "Starting to train on instance 159\n",
      "could not train on instance 159\n",
      "Starting to train on instance 160\n",
      "loss: 14.402215  [  160/  800]\n",
      "Starting to train on instance 161\n",
      "Starting to train on instance 162\n",
      "Starting to train on instance 163\n",
      "Starting to train on instance 164\n",
      "Starting to train on instance 165\n",
      "Starting to train on instance 166\n",
      "Starting to train on instance 167\n",
      "Starting to train on instance 168\n",
      "Starting to train on instance 169\n",
      "Starting to train on instance 170\n",
      "loss: 13.381169  [  170/  800]\n",
      "Starting to train on instance 171\n",
      "Starting to train on instance 172\n",
      "could not train on instance 172\n",
      "Starting to train on instance 173\n",
      "could not train on instance 173\n",
      "Starting to train on instance 174\n",
      "Starting to train on instance 175\n",
      "Starting to train on instance 176\n",
      "could not train on instance 176\n",
      "Starting to train on instance 177\n",
      "Starting to train on instance 178\n",
      "Starting to train on instance 179\n",
      "Starting to train on instance 180\n",
      "loss: 13.908775  [  180/  800]\n",
      "Starting to train on instance 181\n",
      "Starting to train on instance 182\n",
      "Starting to train on instance 183\n",
      "Starting to train on instance 184\n",
      "Starting to train on instance 185\n",
      "Starting to train on instance 186\n",
      "Starting to train on instance 187\n",
      "Starting to train on instance 188\n",
      "Starting to train on instance 189\n",
      "Starting to train on instance 190\n",
      "loss: 13.606552  [  190/  800]\n",
      "Starting to train on instance 191\n",
      "Starting to train on instance 192\n",
      "Starting to train on instance 193\n",
      "Starting to train on instance 194\n",
      "Starting to train on instance 195\n",
      "Starting to train on instance 196\n",
      "Starting to train on instance 197\n",
      "Starting to train on instance 198\n",
      "Starting to train on instance 199\n",
      "Starting to train on instance 200\n",
      "loss: 13.674438  [  200/  800]\n",
      "Starting to train on instance 201\n",
      "Starting to train on instance 202\n",
      "Starting to train on instance 203\n",
      "Starting to train on instance 204\n",
      "Starting to train on instance 205\n",
      "Starting to train on instance 206\n",
      "Starting to train on instance 207\n",
      "Starting to train on instance 208\n",
      "Starting to train on instance 209\n",
      "Starting to train on instance 210\n",
      "loss: 13.370179  [  210/  800]\n",
      "Starting to train on instance 211\n",
      "Starting to train on instance 212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train on instance 213\n",
      "Starting to train on instance 214\n",
      "Starting to train on instance 215\n",
      "Starting to train on instance 216\n",
      "Starting to train on instance 217\n",
      "Starting to train on instance 218\n",
      "Starting to train on instance 219\n",
      "Starting to train on instance 220\n",
      "loss: 13.826776  [  220/  800]\n",
      "Starting to train on instance 221\n",
      "Starting to train on instance 222\n",
      "Starting to train on instance 223\n",
      "Starting to train on instance 224\n",
      "Starting to train on instance 225\n",
      "Starting to train on instance 226\n",
      "could not train on instance 226\n",
      "Starting to train on instance 227\n",
      "Starting to train on instance 228\n",
      "Starting to train on instance 229\n",
      "Starting to train on instance 230\n",
      "loss: 13.509699  [  230/  800]\n",
      "Starting to train on instance 231\n",
      "Starting to train on instance 232\n",
      "Starting to train on instance 233\n",
      "Starting to train on instance 234\n",
      "Starting to train on instance 235\n",
      "Starting to train on instance 236\n",
      "Starting to train on instance 237\n",
      "Starting to train on instance 238\n",
      "Starting to train on instance 239\n",
      "Starting to train on instance 240\n",
      "loss: 13.136269  [  240/  800]\n",
      "Starting to train on instance 241\n",
      "could not train on instance 241\n",
      "Starting to train on instance 242\n",
      "Starting to train on instance 243\n",
      "Starting to train on instance 244\n",
      "Starting to train on instance 245\n",
      "Starting to train on instance 246\n",
      "Starting to train on instance 247\n",
      "Starting to train on instance 248\n",
      "Starting to train on instance 249\n",
      "Starting to train on instance 250\n",
      "loss: 14.043928  [  250/  800]\n",
      "Starting to train on instance 251\n",
      "Starting to train on instance 252\n",
      "Starting to train on instance 253\n",
      "Starting to train on instance 254\n",
      "Starting to train on instance 255\n",
      "Starting to train on instance 256\n",
      "Starting to train on instance 257\n",
      "Starting to train on instance 258\n",
      "Starting to train on instance 259\n",
      "Starting to train on instance 260\n",
      "loss: 13.894978  [  260/  800]\n",
      "Starting to train on instance 261\n",
      "Starting to train on instance 262\n",
      "Starting to train on instance 263\n",
      "Starting to train on instance 264\n",
      "Starting to train on instance 265\n",
      "Starting to train on instance 266\n",
      "Starting to train on instance 267\n",
      "Starting to train on instance 268\n",
      "Starting to train on instance 269\n",
      "Starting to train on instance 270\n",
      "loss: 13.482637  [  270/  800]\n",
      "Starting to train on instance 271\n",
      "Starting to train on instance 272\n",
      "Starting to train on instance 273\n",
      "Starting to train on instance 274\n",
      "Starting to train on instance 275\n",
      "Starting to train on instance 276\n",
      "Starting to train on instance 277\n",
      "Starting to train on instance 278\n",
      "Starting to train on instance 279\n",
      "Starting to train on instance 280\n",
      "loss: 12.966273  [  280/  800]\n",
      "Starting to train on instance 281\n",
      "could not train on instance 281\n",
      "Starting to train on instance 282\n",
      "Starting to train on instance 283\n",
      "Starting to train on instance 284\n",
      "could not train on instance 284\n",
      "Starting to train on instance 285\n",
      "Starting to train on instance 286\n",
      "Starting to train on instance 287\n",
      "Starting to train on instance 288\n",
      "Starting to train on instance 289\n",
      "Starting to train on instance 290\n",
      "loss: 13.409477  [  290/  800]\n",
      "Starting to train on instance 291\n",
      "Starting to train on instance 292\n",
      "Starting to train on instance 293\n",
      "could not train on instance 293\n",
      "Starting to train on instance 294\n",
      "Starting to train on instance 295\n",
      "Starting to train on instance 296\n",
      "Starting to train on instance 297\n",
      "Starting to train on instance 298\n",
      "Starting to train on instance 299\n",
      "Starting to train on instance 300\n",
      "loss: 14.187302  [  300/  800]\n",
      "Starting to train on instance 301\n",
      "could not train on instance 301\n",
      "Starting to train on instance 302\n",
      "Starting to train on instance 303\n",
      "Starting to train on instance 304\n",
      "Starting to train on instance 305\n",
      "could not train on instance 305\n",
      "Starting to train on instance 306\n",
      "Starting to train on instance 307\n",
      "Starting to train on instance 308\n",
      "Starting to train on instance 309\n",
      "Starting to train on instance 310\n",
      "loss: 14.074363  [  310/  800]\n",
      "Starting to train on instance 311\n",
      "Starting to train on instance 312\n",
      "Starting to train on instance 313\n",
      "Starting to train on instance 314\n",
      "could not train on instance 314\n",
      "Starting to train on instance 315\n",
      "Starting to train on instance 316\n",
      "Starting to train on instance 317\n",
      "Starting to train on instance 318\n",
      "Starting to train on instance 319\n",
      "Starting to train on instance 320\n",
      "loss: 13.971662  [  320/  800]\n",
      "Starting to train on instance 321\n",
      "Starting to train on instance 322\n",
      "Starting to train on instance 323\n",
      "Starting to train on instance 324\n",
      "Starting to train on instance 325\n",
      "Starting to train on instance 326\n",
      "Starting to train on instance 327\n",
      "Starting to train on instance 328\n",
      "Starting to train on instance 329\n",
      "Starting to train on instance 330\n",
      "loss: 13.483962  [  330/  800]\n",
      "Starting to train on instance 331\n",
      "Starting to train on instance 332\n",
      "Starting to train on instance 333\n",
      "Starting to train on instance 334\n",
      "Starting to train on instance 335\n",
      "Starting to train on instance 336\n",
      "Starting to train on instance 337\n",
      "Starting to train on instance 338\n",
      "Starting to train on instance 339\n",
      "Starting to train on instance 340\n",
      "loss: 13.331254  [  340/  800]\n",
      "Starting to train on instance 341\n",
      "could not train on instance 341\n",
      "Starting to train on instance 342\n",
      "Starting to train on instance 343\n",
      "Starting to train on instance 344\n",
      "Starting to train on instance 345\n",
      "Starting to train on instance 346\n",
      "Starting to train on instance 347\n",
      "Starting to train on instance 348\n",
      "Starting to train on instance 349\n",
      "Starting to train on instance 350\n",
      "loss: 13.687362  [  350/  800]\n",
      "Starting to train on instance 351\n",
      "Starting to train on instance 352\n",
      "Starting to train on instance 353\n",
      "Starting to train on instance 354\n",
      "Starting to train on instance 355\n",
      "Starting to train on instance 356\n",
      "Starting to train on instance 357\n",
      "Starting to train on instance 358\n",
      "Starting to train on instance 359\n",
      "Starting to train on instance 360\n",
      "loss: 14.390991  [  360/  800]\n",
      "Starting to train on instance 361\n",
      "could not train on instance 361\n",
      "Starting to train on instance 362\n",
      "Starting to train on instance 363\n",
      "Starting to train on instance 364\n",
      "Starting to train on instance 365\n",
      "Starting to train on instance 366\n",
      "Starting to train on instance 367\n",
      "Starting to train on instance 368\n",
      "Starting to train on instance 369\n",
      "Starting to train on instance 370\n",
      "loss: 13.580200  [  370/  800]\n",
      "Starting to train on instance 371\n",
      "Starting to train on instance 372\n",
      "Starting to train on instance 373\n",
      "Starting to train on instance 374\n",
      "Starting to train on instance 375\n",
      "Starting to train on instance 376\n",
      "could not train on instance 376\n",
      "Starting to train on instance 377\n",
      "Starting to train on instance 378\n",
      "Starting to train on instance 379\n",
      "Starting to train on instance 380\n",
      "loss: 13.284553  [  380/  800]\n",
      "Starting to train on instance 381\n",
      "Starting to train on instance 382\n",
      "Starting to train on instance 383\n",
      "Starting to train on instance 384\n",
      "Starting to train on instance 385\n",
      "Starting to train on instance 386\n",
      "Starting to train on instance 387\n",
      "Starting to train on instance 388\n",
      "Starting to train on instance 389\n",
      "Starting to train on instance 390\n",
      "loss: 13.936901  [  390/  800]\n",
      "Starting to train on instance 391\n",
      "Starting to train on instance 392\n",
      "Starting to train on instance 393\n",
      "Starting to train on instance 394\n",
      "Starting to train on instance 395\n",
      "Starting to train on instance 396\n",
      "Starting to train on instance 397\n",
      "Starting to train on instance 398\n",
      "Starting to train on instance 399\n",
      "Starting to train on instance 400\n",
      "loss: 13.938635  [  400/  800]\n",
      "Starting to train on instance 401\n",
      "Starting to train on instance 402\n",
      "Starting to train on instance 403\n",
      "Starting to train on instance 404\n",
      "Starting to train on instance 405\n",
      "Starting to train on instance 406\n",
      "Starting to train on instance 407\n",
      "Starting to train on instance 408\n",
      "Starting to train on instance 409\n",
      "Starting to train on instance 410\n",
      "loss: 13.323353  [  410/  800]\n",
      "Starting to train on instance 411\n",
      "Starting to train on instance 412\n",
      "Starting to train on instance 413\n",
      "Starting to train on instance 414\n",
      "Starting to train on instance 415\n",
      "Starting to train on instance 416\n",
      "Starting to train on instance 417\n",
      "Starting to train on instance 418\n",
      "Starting to train on instance 419\n",
      "Starting to train on instance 420\n",
      "loss: 13.369698  [  420/  800]\n",
      "Starting to train on instance 421\n",
      "Starting to train on instance 422\n",
      "Starting to train on instance 423\n",
      "Starting to train on instance 424\n",
      "Starting to train on instance 425\n",
      "Starting to train on instance 426\n",
      "Starting to train on instance 427\n",
      "Starting to train on instance 428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train on instance 429\n",
      "Starting to train on instance 430\n",
      "loss: 13.244860  [  430/  800]\n",
      "Starting to train on instance 431\n",
      "Starting to train on instance 432\n",
      "Starting to train on instance 433\n",
      "Starting to train on instance 434\n",
      "Starting to train on instance 435\n",
      "Starting to train on instance 436\n",
      "Starting to train on instance 437\n",
      "Starting to train on instance 438\n",
      "Starting to train on instance 439\n",
      "Starting to train on instance 440\n",
      "loss: 13.569084  [  440/  800]\n",
      "Starting to train on instance 441\n",
      "Starting to train on instance 442\n",
      "Starting to train on instance 443\n",
      "Starting to train on instance 444\n",
      "Starting to train on instance 445\n",
      "Starting to train on instance 446\n",
      "Starting to train on instance 447\n",
      "could not train on instance 447\n",
      "Starting to train on instance 448\n",
      "Starting to train on instance 449\n",
      "Starting to train on instance 450\n",
      "loss: 13.160810  [  450/  800]\n",
      "Starting to train on instance 451\n",
      "Starting to train on instance 452\n",
      "Starting to train on instance 453\n",
      "Starting to train on instance 454\n",
      "Starting to train on instance 455\n",
      "Starting to train on instance 456\n",
      "Starting to train on instance 457\n",
      "Starting to train on instance 458\n",
      "Starting to train on instance 459\n",
      "Starting to train on instance 460\n",
      "loss: 14.015047  [  460/  800]\n",
      "Starting to train on instance 461\n",
      "Starting to train on instance 462\n",
      "Starting to train on instance 463\n",
      "Starting to train on instance 464\n",
      "Starting to train on instance 465\n",
      "Starting to train on instance 466\n",
      "Starting to train on instance 467\n",
      "Starting to train on instance 468\n",
      "could not train on instance 468\n",
      "Starting to train on instance 469\n",
      "Starting to train on instance 470\n",
      "loss: 13.309925  [  470/  800]\n",
      "Starting to train on instance 471\n",
      "Starting to train on instance 472\n",
      "Starting to train on instance 473\n",
      "Starting to train on instance 474\n",
      "Starting to train on instance 475\n",
      "Starting to train on instance 476\n",
      "Starting to train on instance 477\n",
      "Starting to train on instance 478\n",
      "Starting to train on instance 479\n",
      "Starting to train on instance 480\n",
      "loss: 14.297591  [  480/  800]\n",
      "Starting to train on instance 481\n",
      "Starting to train on instance 482\n",
      "could not train on instance 482\n",
      "Starting to train on instance 483\n",
      "Starting to train on instance 484\n",
      "Starting to train on instance 485\n",
      "Starting to train on instance 486\n",
      "Starting to train on instance 487\n",
      "Starting to train on instance 488\n",
      "Starting to train on instance 489\n",
      "Starting to train on instance 490\n",
      "loss: 13.919324  [  490/  800]\n",
      "Starting to train on instance 491\n",
      "could not train on instance 491\n",
      "Starting to train on instance 492\n",
      "could not train on instance 492\n",
      "Starting to train on instance 493\n",
      "Starting to train on instance 494\n",
      "Starting to train on instance 495\n",
      "Starting to train on instance 496\n",
      "Starting to train on instance 497\n",
      "Starting to train on instance 498\n",
      "Starting to train on instance 499\n",
      "Starting to train on instance 500\n",
      "loss: 14.224254  [  500/  800]\n",
      "Starting to train on instance 501\n",
      "Starting to train on instance 502\n",
      "Starting to train on instance 503\n",
      "Starting to train on instance 504\n",
      "Starting to train on instance 505\n",
      "could not train on instance 505\n",
      "Starting to train on instance 506\n",
      "Starting to train on instance 507\n",
      "Starting to train on instance 508\n",
      "Starting to train on instance 509\n",
      "could not train on instance 509\n",
      "Starting to train on instance 510\n",
      "loss: 13.505582  [  510/  800]\n",
      "Starting to train on instance 511\n",
      "Starting to train on instance 512\n",
      "Starting to train on instance 513\n",
      "Starting to train on instance 514\n",
      "Starting to train on instance 515\n",
      "Starting to train on instance 516\n",
      "Starting to train on instance 517\n",
      "Starting to train on instance 518\n",
      "Starting to train on instance 519\n",
      "Starting to train on instance 520\n",
      "loss: 13.165174  [  520/  800]\n",
      "Starting to train on instance 521\n",
      "Starting to train on instance 522\n",
      "Starting to train on instance 523\n",
      "Starting to train on instance 524\n",
      "Starting to train on instance 525\n",
      "Starting to train on instance 526\n",
      "Starting to train on instance 527\n",
      "Starting to train on instance 528\n",
      "Starting to train on instance 529\n",
      "Starting to train on instance 530\n",
      "loss: 13.586052  [  530/  800]\n",
      "Starting to train on instance 531\n",
      "Starting to train on instance 532\n",
      "could not train on instance 532\n",
      "Starting to train on instance 533\n",
      "could not train on instance 533\n",
      "Starting to train on instance 534\n",
      "Starting to train on instance 535\n",
      "Starting to train on instance 536\n",
      "Starting to train on instance 537\n",
      "Starting to train on instance 538\n",
      "could not train on instance 538\n",
      "Starting to train on instance 539\n",
      "Starting to train on instance 540\n",
      "loss: 13.703554  [  540/  800]\n",
      "Starting to train on instance 541\n",
      "Starting to train on instance 542\n",
      "Starting to train on instance 543\n",
      "Starting to train on instance 544\n",
      "Starting to train on instance 545\n",
      "Starting to train on instance 546\n",
      "Starting to train on instance 547\n",
      "Starting to train on instance 548\n",
      "Starting to train on instance 549\n",
      "Starting to train on instance 550\n",
      "loss: 13.746666  [  550/  800]\n",
      "Starting to train on instance 551\n",
      "Starting to train on instance 552\n",
      "Starting to train on instance 553\n",
      "Starting to train on instance 554\n",
      "could not train on instance 554\n",
      "Starting to train on instance 555\n",
      "Starting to train on instance 556\n",
      "Starting to train on instance 557\n",
      "Starting to train on instance 558\n",
      "Starting to train on instance 559\n",
      "Starting to train on instance 560\n",
      "loss: 13.385921  [  560/  800]\n",
      "Starting to train on instance 561\n",
      "Starting to train on instance 562\n",
      "could not train on instance 562\n",
      "Starting to train on instance 563\n",
      "Starting to train on instance 564\n",
      "Starting to train on instance 565\n",
      "Starting to train on instance 566\n",
      "Starting to train on instance 567\n",
      "Starting to train on instance 568\n",
      "Starting to train on instance 569\n",
      "Starting to train on instance 570\n",
      "loss: 14.005804  [  570/  800]\n",
      "Starting to train on instance 571\n",
      "Starting to train on instance 572\n",
      "Starting to train on instance 573\n",
      "Starting to train on instance 574\n",
      "Starting to train on instance 575\n",
      "Starting to train on instance 576\n",
      "Starting to train on instance 577\n",
      "Starting to train on instance 578\n",
      "Starting to train on instance 579\n",
      "Starting to train on instance 580\n",
      "loss: 13.374734  [  580/  800]\n",
      "Starting to train on instance 581\n",
      "Starting to train on instance 582\n",
      "Starting to train on instance 583\n",
      "Starting to train on instance 584\n",
      "Starting to train on instance 585\n",
      "Starting to train on instance 586\n",
      "Starting to train on instance 587\n",
      "Starting to train on instance 588\n",
      "Starting to train on instance 589\n",
      "Starting to train on instance 590\n",
      "could not train on instance 590\n",
      "Starting to train on instance 591\n",
      "Starting to train on instance 592\n",
      "Starting to train on instance 593\n",
      "Starting to train on instance 594\n",
      "Starting to train on instance 595\n",
      "could not train on instance 595\n",
      "Starting to train on instance 596\n",
      "Starting to train on instance 597\n",
      "Starting to train on instance 598\n",
      "Starting to train on instance 599\n",
      "could not train on instance 599\n",
      "Starting to train on instance 600\n",
      "loss: 14.082438  [  600/  800]\n",
      "Starting to train on instance 601\n",
      "Starting to train on instance 602\n",
      "Starting to train on instance 603\n",
      "Starting to train on instance 604\n",
      "Starting to train on instance 605\n",
      "Starting to train on instance 606\n",
      "Starting to train on instance 607\n",
      "Starting to train on instance 608\n",
      "Starting to train on instance 609\n",
      "could not train on instance 609\n",
      "Starting to train on instance 610\n",
      "loss: 13.368234  [  610/  800]\n",
      "Starting to train on instance 611\n",
      "Starting to train on instance 612\n",
      "Starting to train on instance 613\n",
      "Starting to train on instance 614\n",
      "Starting to train on instance 615\n",
      "Starting to train on instance 616\n",
      "Starting to train on instance 617\n",
      "Starting to train on instance 618\n",
      "Starting to train on instance 619\n",
      "Starting to train on instance 620\n",
      "loss: 13.200780  [  620/  800]\n",
      "Starting to train on instance 621\n",
      "Starting to train on instance 622\n",
      "could not train on instance 622\n",
      "Starting to train on instance 623\n",
      "Starting to train on instance 624\n",
      "Starting to train on instance 625\n",
      "Starting to train on instance 626\n",
      "Starting to train on instance 627\n",
      "Starting to train on instance 628\n",
      "could not train on instance 628\n",
      "Starting to train on instance 629\n",
      "Starting to train on instance 630\n",
      "loss: 13.801929  [  630/  800]\n",
      "Starting to train on instance 631\n",
      "Starting to train on instance 632\n",
      "Starting to train on instance 633\n",
      "Starting to train on instance 634\n",
      "Starting to train on instance 635\n",
      "Starting to train on instance 636\n",
      "Starting to train on instance 637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train on instance 638\n",
      "Starting to train on instance 639\n",
      "Starting to train on instance 640\n",
      "loss: 13.273820  [  640/  800]\n",
      "Starting to train on instance 641\n",
      "Starting to train on instance 642\n",
      "Starting to train on instance 643\n",
      "could not train on instance 643\n",
      "Starting to train on instance 644\n",
      "Starting to train on instance 645\n",
      "Starting to train on instance 646\n",
      "Starting to train on instance 647\n",
      "Starting to train on instance 648\n",
      "Starting to train on instance 649\n",
      "Starting to train on instance 650\n",
      "loss: 13.763478  [  650/  800]\n",
      "Starting to train on instance 651\n",
      "Starting to train on instance 652\n",
      "Starting to train on instance 653\n",
      "Starting to train on instance 654\n",
      "Starting to train on instance 655\n",
      "Starting to train on instance 656\n",
      "Starting to train on instance 657\n",
      "Starting to train on instance 658\n",
      "Starting to train on instance 659\n",
      "Starting to train on instance 660\n",
      "could not train on instance 660\n",
      "Starting to train on instance 661\n",
      "Starting to train on instance 662\n",
      "Starting to train on instance 663\n",
      "Starting to train on instance 664\n",
      "Starting to train on instance 665\n",
      "Starting to train on instance 666\n",
      "Starting to train on instance 667\n",
      "Starting to train on instance 668\n",
      "Starting to train on instance 669\n",
      "Starting to train on instance 670\n",
      "loss: 13.024328  [  670/  800]\n",
      "Starting to train on instance 671\n",
      "could not train on instance 671\n",
      "Starting to train on instance 672\n",
      "Starting to train on instance 673\n",
      "Starting to train on instance 674\n",
      "Starting to train on instance 675\n",
      "Starting to train on instance 676\n",
      "Starting to train on instance 677\n",
      "Starting to train on instance 678\n",
      "Starting to train on instance 679\n",
      "Starting to train on instance 680\n",
      "loss: 14.306878  [  680/  800]\n",
      "Starting to train on instance 681\n",
      "Starting to train on instance 682\n",
      "Starting to train on instance 683\n",
      "Starting to train on instance 684\n",
      "Starting to train on instance 685\n",
      "Starting to train on instance 686\n",
      "Starting to train on instance 687\n",
      "Starting to train on instance 688\n",
      "Starting to train on instance 689\n",
      "Starting to train on instance 690\n",
      "loss: 13.208542  [  690/  800]\n",
      "Starting to train on instance 691\n",
      "Starting to train on instance 692\n",
      "Starting to train on instance 693\n",
      "could not train on instance 693\n",
      "Starting to train on instance 694\n",
      "Starting to train on instance 695\n",
      "Starting to train on instance 696\n",
      "Starting to train on instance 697\n",
      "Starting to train on instance 698\n",
      "Starting to train on instance 699\n",
      "Starting to train on instance 700\n",
      "loss: 13.873955  [  700/  800]\n",
      "Starting to train on instance 701\n",
      "Starting to train on instance 702\n",
      "Starting to train on instance 703\n",
      "could not train on instance 703\n",
      "Starting to train on instance 704\n",
      "Starting to train on instance 705\n",
      "Starting to train on instance 706\n",
      "Starting to train on instance 707\n",
      "could not train on instance 707\n",
      "Starting to train on instance 708\n",
      "Starting to train on instance 709\n",
      "Starting to train on instance 710\n",
      "loss: 13.176589  [  710/  800]\n",
      "Starting to train on instance 711\n",
      "Starting to train on instance 712\n",
      "Starting to train on instance 713\n",
      "Starting to train on instance 714\n",
      "Starting to train on instance 715\n",
      "could not train on instance 715\n",
      "Starting to train on instance 716\n",
      "Starting to train on instance 717\n",
      "Starting to train on instance 718\n",
      "Starting to train on instance 719\n",
      "Starting to train on instance 720\n",
      "loss: 13.592716  [  720/  800]\n",
      "Starting to train on instance 721\n",
      "Starting to train on instance 722\n",
      "Starting to train on instance 723\n",
      "Starting to train on instance 724\n",
      "Starting to train on instance 725\n",
      "Starting to train on instance 726\n",
      "Starting to train on instance 727\n",
      "Starting to train on instance 728\n",
      "Starting to train on instance 729\n",
      "Starting to train on instance 730\n",
      "loss: 14.439924  [  730/  800]\n",
      "Starting to train on instance 731\n",
      "Starting to train on instance 732\n",
      "Starting to train on instance 733\n",
      "Starting to train on instance 734\n",
      "Starting to train on instance 735\n",
      "Starting to train on instance 736\n",
      "Starting to train on instance 737\n",
      "Starting to train on instance 738\n",
      "Starting to train on instance 739\n",
      "Starting to train on instance 740\n",
      "loss: 13.759439  [  740/  800]\n",
      "Starting to train on instance 741\n",
      "Starting to train on instance 742\n",
      "Starting to train on instance 743\n",
      "Starting to train on instance 744\n",
      "Starting to train on instance 745\n",
      "could not train on instance 745\n",
      "Starting to train on instance 746\n",
      "Starting to train on instance 747\n",
      "Starting to train on instance 748\n",
      "could not train on instance 748\n",
      "Starting to train on instance 749\n",
      "Starting to train on instance 750\n",
      "could not train on instance 750\n",
      "Starting to train on instance 751\n",
      "Starting to train on instance 752\n",
      "Starting to train on instance 753\n",
      "Starting to train on instance 754\n",
      "Starting to train on instance 755\n",
      "Starting to train on instance 756\n",
      "Starting to train on instance 757\n",
      "could not train on instance 757\n",
      "Starting to train on instance 758\n",
      "could not train on instance 758\n",
      "Starting to train on instance 759\n",
      "Starting to train on instance 760\n",
      "loss: 13.563697  [  760/  800]\n",
      "Starting to train on instance 761\n",
      "Starting to train on instance 762\n",
      "Starting to train on instance 763\n",
      "Starting to train on instance 764\n",
      "Starting to train on instance 765\n",
      "Starting to train on instance 766\n",
      "Starting to train on instance 767\n",
      "Starting to train on instance 768\n",
      "Starting to train on instance 769\n",
      "Starting to train on instance 770\n",
      "loss: 13.880515  [  770/  800]\n",
      "Starting to train on instance 771\n",
      "Starting to train on instance 772\n",
      "Starting to train on instance 773\n",
      "Starting to train on instance 774\n",
      "Starting to train on instance 775\n",
      "could not train on instance 775\n",
      "Starting to train on instance 776\n",
      "Starting to train on instance 777\n",
      "Starting to train on instance 778\n",
      "Starting to train on instance 779\n",
      "Starting to train on instance 780\n",
      "loss: 14.013720  [  780/  800]\n",
      "Starting to train on instance 781\n",
      "could not train on instance 781\n",
      "Starting to train on instance 782\n",
      "Starting to train on instance 783\n",
      "Starting to train on instance 784\n",
      "Starting to train on instance 785\n",
      "Starting to train on instance 786\n",
      "Starting to train on instance 787\n",
      "Starting to train on instance 788\n",
      "Starting to train on instance 789\n",
      "Starting to train on instance 790\n",
      "loss: 13.543229  [  790/  800]\n",
      "Starting to train on instance 791\n",
      "could not train on instance 791\n",
      "Starting to train on instance 792\n",
      "Starting to train on instance 793\n",
      "Starting to train on instance 794\n",
      "Starting to train on instance 795\n",
      "Starting to train on instance 796\n",
      "Starting to train on instance 797\n",
      "Starting to train on instance 798\n",
      "Starting to train on instance 799\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Rprop\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "\n",
    "unsat_csp = BinAtCSP()\n",
    "unsat_csp.from_cnf(unsat_cnf)\n",
    "\n",
    "num_vars = len(unsat_csp.variables)\n",
    "\n",
    "m = Model()\n",
    "\n",
    "loss_fn = UnsupervisedLoss(number_samples=50)\n",
    "optimizer = torch.optim.Rprop(params=m.parameters(), lr=learning_rate) \n",
    "\n",
    "def train():\n",
    "    for (idx, instance) in enumerate(train_instances):\n",
    "        print(f\"Starting to train on instance {idx}\")\n",
    "        \n",
    "        # print([c.support for c in instance.constraints])\n",
    "        try:\n",
    "            pred = m(instance.to_pytorch())\n",
    "            loss = loss_fn(pred, instance)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if idx % 10 == 0:\n",
    "                loss = loss.item()\n",
    "                print(f\"loss: {loss:>7f}  [{idx:>5d}/{len(train_data):>5d}]\")\n",
    "        except:\n",
    "            print(f\"could not train on instance {idx}\")\n",
    "                \n",
    "train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a6eb3b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Evaluate\n",
    "\n",
    "To evaluate the performance, we compare the trained model with a naive Moser's algorithm, to see whether it performs any better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "b6cbe066",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_instances, steps_eval):\n",
    "    results = []\n",
    "    for i in test_instances[:100]:\n",
    "        \n",
    "        pred = model(i.to_pytorch())\n",
    "        variable_params = pred[:len(i.variables)]\n",
    "        trained_oracle = Oracle(Bernoulli, params=variable_params)\n",
    "        trained_walker = OracleRandomWalker(trained_oracle, i)\n",
    "        \n",
    "    \n",
    "        random_oracle = Oracle(Bernoulli, params=torch.full_like(variable_params, 0.5))\n",
    "        mosers = OracleRandomWalker(random_oracle, i)\n",
    "        \n",
    "\n",
    "        moser_out, _, _, _ = mosers.run(step_limit=steps_eval)\n",
    "        trained_out, _, _, _ = trained_walker.run(step_limit=steps_eval)\n",
    "        \n",
    "        results.append(np.divide(energy(trained_out, i)**2,energy(moser_out, i)**2))\n",
    "    \n",
    "    print(np.sum(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "id": "966571f4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4835164835164835e+24\n"
     ]
    }
   ],
   "source": [
    "evaluate(m, test_instances, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7badd377",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 18:29:29) \n[Clang 12.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "cd78fef2128015050713e82ca51c6520b11aee7c9ee8df750520bbbc7384cbaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
